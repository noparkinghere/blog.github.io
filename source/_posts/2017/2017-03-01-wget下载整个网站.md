---
layout: "post"
title: "wget下载整个网站"
date: "2017-03-01 10:43"
---

当我们看到一个优秀的网站，但上网却不是太方便的时候，往往将网页保存下来，然后离线浏览是最佳选择，但是对于一些教程，因为目录下的文章较多，有的动辄上百篇文章，无疑一个个保存网页肯定效率低下不现实。这边我们推荐使用 wget 这个工具来获取保存整个网站/目录下的内容。

需要下载某个目录下面的所有文件。命令如下

`wget -c -r -np -k -L -p www.xxx.org/pub/path/`

在下载时。有用到外部域名的图片或连接。如果需要同时下载就要用-H参数。

例如： `wget -c -r -np -k -L -p http://akaedu.github.io/book/`

或者也可以使用如下方式：

wget --no-check-certificate --recursive --no-clobber --domains website.org --no-parent www.website.org/xxx

--recursive 现在整个网站

--domains 指定下载的域，将不会下载其它域的文件

​--no-parent 不下载父链接的内容，很重要，能减少不必要的循环

--no-clobber 不重复下载一个文件​

​--no-check-certificate 不检查网站认证信息


> 参考链接：
> http://blog.sina.com.cn/s/blog_4ddef8f80102vtq2.html
> http://www.cnblogs.com/lidp/archive/2010/03/02/1696447.html
